{"about":{"title":"About Me","links":[],"tags":[],"content":"I am a Senior Researcher in Trustworthy Systems at the Alan Turing Institute, in the Tools, Practices, and Systems programme. I lead the Innovation and Impact Hub for the Turing Research and Innovation Cluster in Digital Twins. I am also the Principal Investigator for the Trustworthy and Ethical Assurance of Digital Twins project (funded by UKRI), and project lead for the Trustworthy and Ethical Assurance (TEA) Platform.\nMy research expertise includes trustworthy AI systems 🤝, digital twinning 🔁, responsible research and innovation ⚖️, data and AI ethics 🤖, and philosophy of cognitive science 🧠. I currently lead the Trustworthy and Ethical Assurance of Digital Twins (TEA-DT) project, supported by a grant from the UKRI’s BRAID programme.\nMy research has been featured in the Conversation, the Guardian, BBC Radio 4, the New York Times and Vox. I have also advised numerous regulators and policy-makers and worked with the Ministry of Justice; Office for Artificial Intelligence; Information Commissioner’s Office; Centre for Data Ethics and Innovation; the Department for Science, Innovation, and Technology; and the Department of Health and Social Care.\nBut my most important research is the constant learning involved with being a loving father and husband 👨‍👩‍👧! When I’m not figuring out how to do the above, you can find me at the bouldering gym figuring out how to ascend a bouldering problem 🧗\nBackground §\nPrior to joining the Alan Turing Institute, I was a Postdoctoral Research Associate at the Oxford Internet Institute, University of Oxford. Before this, I worked at the Department of Computer Science, University of Bristol where I explored the ethical and epistemological impact of big data and artificial intelligence as part of the thinkBIG project.\nI completed my PhD in 2017 at the Department of Philosophy, University of Bristol. My thesis was on embodied decision-making as understood from the perspective of the predictive processing hypothesis."},"current-projects":{"title":"Current Projects","links":["previous-projects"],"tags":["list"],"content":"On this page, you can find a list of my current projects. Details of my previous projects are also available.\nTrustworthy and Ethical Assurance §\nComing soon.\nInnovation and Impact Hub (TRIC-DT) §\nI lead the Innovation and Impact Hub for the Turing’s Research and Innovation Cluster in Digital Twins (TRIC-DT). The Innovation and Impact Hub helps coordinate and convene the scientific aims and objectives of the TRIC-DT’s three themes—natural environment, health, and infrastructure—by supporting the development of open and reproducible tools, driving impact with diverse stakeholders, building and supporting multi-disciplinary communities of practice, and embedding best practices for responsible research and innovation throughout the TRIC-DT.\nYou can read more about the TRIC-DT and the Innovation and Impact Hub here: https://www.turing.ac.uk/research/research-projects/tric-dt\nTuring Commons §\nI lead the Turing Commons—an online skills and training platform to support open dialogue and reflection about the responsible design, development, and deployment of data-driven technologies. Through this platform, you can access many free resources and content, and also access information about how you can contribute to its ongoing development."},"index":{"title":"Welcome","links":["publications","presentations","teaching-materials","current-projects","past-projects"],"tags":[],"content":"Hello, World 👋\nI am an Senior Researcher in Trustworthy Systems at the Alan Turing Institute, in the Tools, Practices, and Systems programme. I also lead the Innovation and Impact Hub for the Turing’s Research and Innovation Cluster in Digital Twins (TRIC-DT).\nMy research expertise includes trustworthy AI systems 🤝, digital twinning 🔁, responsible research and innovation ⚖️, data and AI ethics 🤖, and philosophy of cognitive science 🧠. I currently lead the Trustworthy and Ethical Assurance of Digital Twins (TEA-DT) project, supported by a grant from the UKRI’s BRAID programme.\nMy research has been featured in the Conversation, the Guardian, BBC Radio 4, the New York Times and Vox. I have also advised numerous regulators and policy-makers and worked with the Ministry of Justice; Office for Artificial Intelligence; Information Commissioner’s Office; Centre for Data Ethics and Innovation; the Department for Science, Innovation, and Technology; and the Department of Health and Social Care.\nBut my most important research is the constant learning involved with being a loving father and husband 👨‍👩‍👧! When I’m not figuring out how to do the above, you can find me at the bouldering gym figuring out how to ascend a bouldering problem 🧗\nAbout this Site §\nThis site is setup as a digital garden. If you are unfamiliar with this concept, I recommend reading this article. Alternatively, just start having a click using the Explorer or the Graph View to browse my extended mind 🧠.\nIf you’re looking for something specific, the following pages may be helpful:\n\nPublications (with and without paywalls 💰)\nPresentations (a list of my talks, including some slides and transcripts)\nTeaching Materials\nCurrent Projects\nPast Projects\n"},"presentations":{"title":"Presentations","links":["reimagining-digital-well-being"],"tags":["list"],"content":"\n\n                  \n                  Presentations \n                  \n                \nOn this page you will find a list of past presentations and (where available) a link to the transcript, slides, and other associated materials.\n\n2024 §\n\nReimagining Digital Well-Being in an Age of Digital Twins (Future of Digital Well-Being Workshop and Conference, Amsterdam)\n\n2023 §\nComing Soon\n2022 §\nComing Soon\n2021 §\n\n‘Ethical Assurance: How to design, develop, and deploy trustworthy systems.’ (DSTL AI Fest 4) \n‘A Citizen’s Guide to Data: Ethical, Social and Legal Issues’ (Resident’s Panel, Camden City Council)\n‘Ethical Assurance of Digital Mental Healthcare’ (TAS Hub, All Hand’s Meeting)\n‘Changing Interface for Mental Healthcare’ (Panel Conversation, Digital Innovation in Mental Health 2021 Conference)\n‘Ethical Assurance of Data-Driven Technology’ (Invited Keynote—Government Data Leaders Forum, Deloitte UK)\n‘AI and Public Policy’ (Oxford UNIQ+)\n‘Participatory Assurance: The Responsible Design, Development, and Deployment of Augmented Intelligence’ (TAS/RUSI Workshop on Defence and Security)\n‘Digital Well-Being: The Ethical Design and Governance of Digital Technologies for Mental Health and Well-Being’ (Invited Lecture—Legal Courses on Law and Technology, European Law Society Association\n\n2020 §\n\n‘Explanation and Active Enquiry’ – (Invited Workshop Paper) Jubilee Centre for Character and Virtues, University of Birmingham\n‘Responsible Research and Innovation in Digital Psychiatry’ - (Invited Seminar) TU Eindhoven, Netherlands\n‘Responsible and Trustworthy Machine Learning’ - (Invited Seminar) University of Bristol, UK\n‘Digital Well-Being’ - (Video Podcast) EAVI Conversations 2020\n‘The Ethics of Digital Well-Being’ - (Public Lecture) Ethical Reading\n‘Living online: the long-term impact on wellbeing’ – (Inquiry Hearing) House of Lords COVID-19 Select Committee\n‘AI and Trust’ (Invited Lecture—Ethical Implications of AI SS2020, Frankfurt Big Data Lab, Goethe University)\n‘Responsibility, Trust, and Assurance: How should we design, develop, and deploy machine learning?’ - (Keynote Lecture) Science and Society Conference, London School of Economics\n‘Digital Psychiatry: Risks and Opportunities for Public Health and Well-Being’ (Invited Lecture, University of Vienna)\n\n2019 §\n\n‘Digital Psychiatry: Risks and Opportunities’ (Invited Presentation—AI@Oxford, University of Oxford)\n‘Exploring the Unknown: The Predictive Mind in Immersive Environments’ - Global Health Film Festival 2019, London, UK\n‘Digital Psychiatry: Risks and Opportunities for Public Health and Well-Being’ (Ethics of Digital Well-Being, University of Oxford)\n‘The Methodology and Ethics of Targeting’ (Invited Presentation—Leverhulme Centre for Intelligence, University of Cambridge)\n\n2018 §\n\n‘An Analysis of the Interaction Between Intelligent Software Agents and Human Users’ — MANCEPT 2018, Paternalism, Nudging and the Digital Sphere (University of Manchester Centre for Political Theory)\n‘An Analysis of the Interaction Between Intelligent Software Agents and Human Users’ — thinkBIG Workshop on the Ethical and Social Challenges posed by Artificial Intelligence (Cumberland Lodge, Windsor)\n‘Building machines that “learn and think” about morality’ — Society for Artificial Intelligence and the Simulation of Behaviour, 2018 (University of Liverpool)\n"},"previous-projects":{"title":"Previous Projects","links":["archive/publications"],"tags":["list"],"content":"\n\n                  \n                  Additional Resources \n                  \n                \nDetails of current projects publications  are also available.\n\nTrustworthy and Ethical Assurance of Digital Health and Healthcare §\nInformation coming soon.\nHuman Rights, Democracy, and the Rule of Law Assurance Framework §\nInformation coming soon.\nTrustworthy Assurance of Digital Mental Healthcare §\nUntil July 2022, I was the Principal Investigator of a UKRI-funded project, supported by the Trustworthy Autonomous Systems Hub, titled ’Trustworthy Assurance of Digital Mental Healthcare’. The goal of this project was to develop a method for assuring ethical goals and claims associated with the design, development, deployment, and use of data-driven technologies in mental healthcare.\nYou can read our final policy report here.\nCo-Developing a Data Charter §\nInformation coming soon.\nFacilitating Responsible Participation in Data Science §\nI was previously a co-organiser of the Special Interest Group at the Alan Turing Institute, ’Facilitating Responsible Participation in Data Science’. This group met regularly to share knowledge and understanding about how to increase participation in data science in a responsible and ethical manner."},"publications":{"title":"Publications","links":[],"tags":["list"],"content":"Articles §\n\nWagg, D., Burr, C., Shepherd, J., Conti, Z. X., Enzer, M., &amp; Niederer, S. (2024). The philosophical foundations of digital twinning. Engineering Archive. https://doi.org/10.31224/3500\nDewhurst, J., &amp; Burr, C. (2022). Normative folk psychology and decision theory. Mind &amp; Language, 37(4), 525–542. https://doi.org/10.1111/mila.12347\nBurr, C., &amp; Leslie, D. (2022). Ethical assurance: A practical approach to the responsible design, development, and deployment of data-driven technologies. AI and Ethics. https://doi.org/10.1007/s43681-022-00178-0\n\nGoogle Drive\narXiv Preprint\n\n\nBurr, C., J. Morley, M. Taddeo, &amp; L. Floridi. (2020). Digital Psychiatry: Risks and Opportunities for Public Health and Wellbeing. IEEE Transactions on Technology and Society, 1(1), 21–33. https://doi.org/10.1109/TTS.2020.2977059\n\nPreprint\n\n\nBurr, C., Taddeo, M., &amp; Floridi, L. (2020). The Ethics of Digital Well-Being: A Thematic Review. Science and Engineering Ethics. doi: 10.1007/s11948-020-00175-8\n\nPreprint\nPublished Version (Open Access)\n\n\nMorley J, Machado CCV, Burr C, Cowls J, Joshi I, Taddeo M, Floridi L. (2020). The ethics of AI in health care: A mapping review. Social Science &amp; Medicine. 260, 113172.\n\nPreprint\nPublished Version (£)\n\n\nBurr, C., &amp; Cristianini, N. (2019). Can Machines Read our Minds? Minds and Machines, 29(3), pp. 461–494.\n\nPreprint\nPublished Version (Open Access)\n\n\nTabor, A., &amp; Burr, C. (2019). Bayesian Learning Models of Pain: A Call to Action.Current Opinion in Behavioral Sciences, 26, pp. 54-61.\n\nPreprint\nPublished Version (£)\n\n\nBurr, C., Cristianini, N. and Ladyman, J. (2018). An Analysis of the Interaction Between Intelligent Software Agents and Human Users. Minds and Machines, 28(4), pp. 735–774.\n\nPreprint\nPublished Version (Open Access)\n\n\nBurr, C. &amp; Jones, M. (2016). Body as Laboratory: Prediction-Error Minimisation, Embodiment and Representation. Philosophical Psychology, 29(4), pp. 586–600.\n\nPreprint\nPublished Version (£)\n\n\n\nEdited Collections §\n\nBurr, C., &amp; Floridi, L. (2020). Ethics of Digital Well-Being. Springer.\n\nDigital Version (£)\nPrint Version\n\n\nBurr, C., &amp; Milano, S. (2020). The 2019 Yearbook of the Digital Ethics Lab. Springer.\n\nDigital Version (£)\n\n\n\nBook Chapters §\n\nKeeling, G., &amp; Burr, C. (2022). Digital manipulation and mental integrity. In The philosophy of online manipulation (pp. 253–271). Routledge.\n\nDigital Version (Open Access)\n\n\nBurr, C., &amp; Floridi, L. (2020). The Ethics of Digital Well-Being: A Multidisciplinary Perspective. In C. Burr and L. Floridi (Eds.) Ethics of Digital Well-Being. Springer.\n\nPreprint\nPublished Version (£)\n\n\nBurr, C., &amp; Morley, J. (2020). ‘Empowerment or Engagement? Digital Health Technologies for Mental Healthcare’. In C. Burr and S. Milano (Eds.) The 2019 Yearbook of the Digital Ethics Lab, pp. 67–88.\n\nPreprint\nPublished Version (£)\n\n\nBurr, C. (2017). “Embodied Decisions and the Predictive Brain”. In Metzinger, T., and Wiese, W. (Eds.) OpenMIND: The Philosophy of Predictive Processing.\n\nPublished Version (Open Access)\n\n\n\nPhD thesis §\n\nBurr, C. (2016). Embodied Decisions and the Predictive Brain. University of Bristol.\n\nPDF\nUniversity of Bristol Record\n\n\n\nConference Proceedings §\n\nBurr, C., &amp; Keeling, G. (2018). Building machines that “learn and think” about morality. In Proceedings of the Society for Artificial Intelligence and the Simulation of Behaviour (AISB 2018).\n\nPreprint\n\n\n\nPolicy Reports and Submissions §\n\nBurr, C., &amp; Powell, R. (2022). Trustworthy Assurance of Digital Mental Healthcare (p. 137). Alan Turing Institute. https://doi.org/10.5281/zenodo.7107200\nAI, human rights, democracy and the rule of law: A primer prepared for the Council of Europe – Alan Turing Institute (2021) \nResponse to National Data Strategy - Alan Turing Institute (2020)\nArtificial Intelligence: How to get it right (Chapter 3: Developing the Governance Framework) - NHSX (2019)\n\nOpinion Pieces §\n\nBurr, C. (2022, March 28). Charities are contributing to growing mistrust of mental-health text support—Here’s why. The Conversation. http://theconversation.com/charities-are-contributing-to-growing-mistrust-of-mental-health-text-support-heres-why-179056\n\nBook Reviews §\n\nBurr, C. (2016). “Review of Unifying the Mind: Cognitive Representations as Graphical Models by David Danks”, Philosophical Psychology, 29(5), pp. 789–791.\n\nPreprint\nPublished Version (£)\n\n\n"},"reimagining-digital-well-being":{"title":"Reimagining Digital Well-Being in an Age of Digital Twins","links":[],"tags":["presentation"],"content":"\n\n                  \n                  Presentation \n                  \n                \nThis talk was first given at the Future of Digital Well-Being Workshop and Conference 2024, organised by Dr Matthew Dennis and held in Amsterdam.\nBurr, C. (2024). Reimagining Digital Well-Being in an Age of Digital Twins. Future of Digital Well-Being Workshop and Conference, Amsterdam, NL. Zenodo. https://doi.org/10.5281/zenodo.10605135\n\n\nIt’s been almost 5 years since Matthew Dennis and I were in contact regarding digital well-being. So, I was delighted when he reached out towards the end of 2023 to ask me to participate in this workshop on the Future of Digital Well-Being.\nHowever, as someone who has not actively researched these topics since 2019, Matthew’s kind invitation posed a conundrum. What could I talk about without just reusing old material that is now likely out of date in such a fast moving field?\nI currently lead the Innovation and Impact Hub for the Alan Turing Institute’s Research and Innovation Cluster in Digital Twins. While digital twins are believed by many to have various benefits for individuals and society, and are currently receiving a lot of funding from national agencies and commercial organisations, I had not considered how they would impact well-being directly.\nHowever, I didn’t want to to turn Matthew’s invitation down, so I took it as a challenge to spend some time considering the links between my present and past research. This is the entry-point for my presentation.\n\nMore specifically, I would like to pose the following question as an anchoring point for reflection and discussion. I won’t be aiming at defending an argument or answer to this question, though I will offer a tentative conclusion towards the end of the presentation.\n\nThe outline for this presentation is as follows:\n\nI will start by returning to some of my previous work to pick out several key concepts that were salient when I was originally working on these topics. \nI will then give a brief introduction to digital twins and digital twinning, for those who are unfamiliar with these concepts.\nFinally, I will conclude with a more philosophical discussion on some important issues and open questions.\n\n\nLet’s begin with the some of the salient topics, as I recall them. \nFirst, back in 2017, when people were beginning to transition from talking about big data as the exciting trend in technology towards AI, I was working with Professor Nello Cristianini and Professor James Layman at the University of Bristol.\nHere, we developed a simple formal model to help identify and analyse some of the ethical challenges associated with “intelligent software agents” 1. This term included recommendation systems, such as Facebook’s news feed or YouTube’s video suggestions, which used relevance feedback from user interactions to steer them towards similar content, to personalised adverts that nudged or persuaded people to click, buy, or engage. Concepts such as control and autonomy were crucial to this work.\nOn the basis of this research we next explored how various methods could be used to infer psychological traits of users, including personality traits, political and sexual orientation, to psychopathologies 2. Again, issues of control arose here, but concepts such as privacy and trust were also at the forefront of concerns.\nFollowing this, I moved to the University of Oxford to work with Professor Luciano Floridi. Here, we focused on “digital well-being” as a concept in its own right, beginning as many research projects do with a literature review of the current landscape 3. The work of people like Rafael Calvo and Dorian Peters, the latter is here today, heavily shaped and influenced our thematic analysis, bringing the concept of self-determination to the centre of my interests.\nThis work continued into a deeper exploration of digital well-being in the context of mental health and healthcare, with a paper that looked at digital psychiatry and drew out 10 lessons that we took to be vital for regulators, policy-makers, designers and developers to consider 4. \nThis research project culminated in a workshop and edited collection, which Matthew kindly contributed to at the time. In the introduction of this collection, we drew out three themes based on the contributed papers: digital gratitude, automating interventions, and sustainable co-well-being 5.\nLet me say something briefly about the second theme. This topic had concerned me a lot since I began working on these topics. As someone who places a strong value on autonomous and informed decision-making, relocating the nexus of agency (including moral agency) in artificial systems (intelligent or otherwise) is not a choice to take lightly. And, it becomes a highly political and social matter when this choice is dominated by a minority of people or groups in society with disproportionate levels of influence.\nMy last real piece of work on these topics was a paper on the Philosophy of Online Manipulation, co-authored with a good friend, Geoff Keeling, who is now at Google [^@keeling2022digital]. Here we explored the idea of manipulation as it pertains to mental integrity—a concept at the time highly under-explored in the literature, despite being a fundamental human right. This paper was completed in 2020, despite the publication only coming out in 2022.\nKeeling, G., &amp; Burr, C. (2022). Digital manipulation and mental integrity. In The philosophy of online manipulation (pp. 253–271). Routledge.\nA lot changed in the intervening years.\n\nThe Covid-19 pandemic and the rise of new forms of AI powered by large language models has undoubtedly changed how we consider, discuss, and understand topics such as digital well-being. I won’t have a lot to say about these events, except for a cursory reflection on multi-agent workflows later in the presentation.\nI expect others, however, will have connected work on digital well-being to these disruptive events, and perhaps will discuss their work later this afternoon.\nHowever, I’d like to try to bridge some of the topics explored in my previous work with the area of research that currently dominates my time and attention: digital twins.\n\nFollowing the pattern of all good philosophers, let’s start with a definition that we can pick at and unpack. As Zhuang et al. note, a digital twin is\n\n“a dynamic model in the virtual world that is fully consistent with its corresponding physical entity in the real world and can simulate its physical counterpart’s characteristics, behaviour, life, and performance in a timely fashion.” 6\n\nThis is one definition among many, but it is characteristic of the many.\nFirst, it acknowledges two crucial components: a virtual model and the physical object or system that is represented. The degree of representation here is important, because digital twins are supposed to be highly realistic and representative of the physical system. Timely collection and processing of high fidelity data, it is assumed, is crucial to this. \nSecond, the utility of the virtual model lies in its ability to provide human users with a means for interacting with a proxy of the physical system, for a variety of purposes. In general, we can capture this utility as informing practical decisions or actions—in other words, interventions, to foreshadow a later critical remark.\n\nDefinitions are helpful, but so are illustrative examples that provide us with an ostensive complement.\nIt would be remiss of me to come to Amsterdam as a representative of the Alan Turing Institute, and not reference the 3D printed stainless steel bridge that spans the Oudezijds Voorburgwal.\nThis bridge was built by the Dutch robotics company, MX3D, in collaboration with researchers at my home institution. When it was still in use, it was embedded with a network of sensors that enabled the bridge to collect data and build a digital twin to keep track of its performance and health.\nOn the right-hand side of this slide is an picture of the CemrgApp—an interactive medical imaging platform that is being used to research cardiac diseases and treatments that will help doctors and patients approach medical decision-making in a more personalised and participatory manner. \nDigital twins for human health and healthcare are an exciting area of research and innovation. Although not shown here, there is also a project led by Peter Coveney at the Centre for Computational Science (UCL) to scale up digital twinning from hearts to digital twins of full humans.\nNeedless to say, a digital twin of a bridge, while highly complex, is nowhere near as complex as a digital twin of a human.\n\nLet’s dig a little bit deeper into the concept of digital twins to help us determine what separates a digital twin from other concepts, such as models or simulations.\nFirst, it is important to note that digital twins are systems. They are not one thing, but can comprise a wide variety of components, sensors, techniques, and processes. This makes their delineation tricky.\nHowever, we can start by acknowledging that a digital twin is not a digital model.\nHere, on the left we have an example of a digital model, which represents the corresponding physical object, but where there is only manual or very loosely coupled data flow between the two objects. For example, a human may observe and measure some properties of the physical object, record data, and use this data to construct a model. This is the traditional relationship that has dominated much of computational science and modelling to date.\nNext, we have what is sometimes referred to as a “digital shadow”. Here, it is expected that there will be an unidirectional flow of data between the physical object and the digital object. For instance, data generated by an IoT sensor that automatically updates a model of a building, such that an alarm could be sounded if a safety-critical variable went out of bounds. However, any actions taken on the basis of this data flow would be decoupled from any subsequent actions or heavily mediated by a human agent.\nFinally, we come to the digital twin—a system that is characterised by tightly coupled and bi-directional data flows, which are primarily if not fully automated. Changes in the physical object are monitored, feed into, and update the digital object, which may be governed by instances of AI. More on this shortly. And then actions are taken on the basis of automated decision-making. For instance, altering the humidity of a smart farm to ensure optimal growth of plants.\nMoreover, the model may be dynamically reconstructed on the basis of the coupled data flows and evolution of the system over time.\nThe astute among you may now be wondering why I referred to the 3D printed bridge as a digital twin. After all, how much change can one expect in a largely fixed and unalterable structure?\n\nLet’s separate out the digital shadow and the digital twin for a moment and explore the main point of difference: the data flow from the digital object to the physical object.\nAt present, a lot of so-called “digital twins” would be best thought of as digital shadows, if we adopt this taxonomy.\n\nHowever, there is no consensus on any taxonomy or conceptual framework for digital twins at present7. Consensus is still emerging.\nSo, what we can refer to as the “degree of coupling” between the digital object and physical object is is highly variable. For instance, a system that has a manual data flow back to the physical object, which takes several months to occur and is managed by humans, is unlikely to be considered a legitimate case of a digital twin. But, what about a situation where the data flow has a human-in-the-loop solely for the purposes of safety assurance?\nAs we extend and reduce the degree of coupling, we will invariably cross a vague and fuzzy conceptual boundary between what some think of as digital shadows and others think of as digital twins.\n\nWithin this vaguely defined region is where we shall return to our initial topic of digital well-being, and where I shall offer a critical remark about the concept of digital twins.\n\nAt present, the idea of ‘digital twins’ and ‘digital twinning’ is largely aspirational.\nThere are few systems that are currently in development or production that would meet strict definitions or criteria for the idealised notion of a digital twin presented on the previous slides. \nHowever, this workshop and conference is about the “future” of digital well-being, so I shall make the most of the philosophical freedom provided to me and explore a hypothetical scenario—albeit one that is rooted in current technologies.\nConsider a digital twin of a person—your own personal assistant, and the sort that those in silicon valley like to pretend are just around the corner as a means to generate hype in foundation models and AI.\nSuch a digital twin would, we can assume, have high-fidelity and near real-time access to myriad personal health and well-being data from you and a host of relevant sources. \nThis data would be used to continuously update the parameters of an associated model, and dynamically reconstruct this personalised model to make recommendations or even take actions on your behalf (e.g. modulating vitamin and mineral composition of your food, adjusting your sleep schedule to ensure “optimal” levels of alertness). \nHealthcare professionals could also access this assistant through secure APIs to run simulations and identify your risk for a variety of illnesses, especially those for which your genetic data suggest a predisposition towards.\nHere we can ask, what are the associated issues with such a scenario, focusing primarily on digital well-being?\n\nThe issue I would like to discuss pertains to self-knowledge and self-determination—a topic that has fascinated us for thousands of years, as evidenced by the famous maxim of “know thyself” that is frequently cited as hanging above the entrance to the famous temple at Delphi in Ancient Greece.\nSelf-knowledge is understood to have clear moral and prudential value. Without self-knowledge, we cannot undertake the sorts of rational and autonomous decisions that are assumed to be characteristic of genuine moral agents, as opposed, say, to children or animals (i.e. moral patients).\nBut, the development and use of a digital twin, such as the one proposed in the hypothetical scenario, would disrupt many of the cognitive, behavioural, and interpersonal dynamics that underpin our capacity for self-determination.\n\nLet’s go back to this diagram again and peek inside the black box of the digital object. As you may recall, I noted that a digital twin is not “one thing”. It is a concept that envelops many constitutive components, processes, and techniques.\nFor instance, the digital object that would analyse and synthesise the data flows from a person (in our hypothetical scenario) could be a highly networked, multi-agent workflow. By this I mean a system of agents, built up from highly-specialised and fine-tuned models of the sorts that are currently being developed by organisations such as OpenAI, Meta, Mistral, and Google.8\n\nI asked one of these sorts of agents, ChatGPT’s DALL-E, to create an image  that captured the vast diversity of microbial life on this planet.\nAnd, then, I asked ChatGPT to create a similar image but this time with the concept of intelligent software agents of the sorts just discussed. Parts of the prompts are displayed below the images for reference.\nThese images are fascinating to consider and ponder. And as someone who is not artistically very competent, I enjoy how such tools can expand my ability to communicate in a different medium. But despite the creative potential, they are still cartoons, and cartoons aren’t typically detailed representations of reality.\n\nThe following quote from Jaron Lanier articulates this concern well.\nA prominent and famously critical voice of novel technologies, Lanier wrote a paper titled, ‘Agents of Alienation’ in which he argued that human reliance on AI to curate and mediate our interaction with information not only leads to a narrowing of human experience but also encourages a form of self-diminishment among users 9.\nIn this paper he fears that such agents, by pretending to know our preferences and serving us a filtered reality, will inevitably dumb down human interaction and creativity. As he states here,\n\n“An agent’s model of what you are interested in will be a cartoon model, and you will see a cartoon version of the world through the agent’s eyes. It is therefore a self-reinforcing model.”\n\nThe observant among you will note that this paper was not published recently, but in 1995. Yet, it’s still salient.\n\nBringing us closer to the present day, Paulan Korenhof et al. write the following in an article that critically analyses the concept of digital twins:\n\n“Due to the affordances of digital data, the digital substitute enables people to monitor, diagnose, and predict aspects of the physical entity at any time and from anywhere […] However, this interaction will always take place with the digital substitute, and not the real physical entity. The risk would be that the more a Digital Twin becomes the primary focus of agents, the more the agents’ attention for the original physical entity may decline or be restricted in time […] Here, a role reversal may occur due to which the substitute becomes the main object of understanding and engagement for agents, while the original physical entity becomes functionally a supplement to the substitute.” 10\n\nLike Lanier, Korenhof et al. critically examine the risk of placing mediating agents between our own perceptual and cognitive capacities and the world itself. \nThis is not a new risk, of course. All technological use—as understood by philosophers such as Don Ihde, Merleau-Ponty, or Donna Harraway—affect our experience and engagement with the world, including ourselves and each other.\nBut the question is whether it is digital twins pose a sui generis risk to our capacity for self-determination?\n\nOr, returning, finally, to my original question, “do digital twins represent a paradigm shift in our understanding of digital well-being”?\nAs I noted at the start, I won’t attempt to answer this fully, but my inclination is “no”.\nThe risk that digital twins pose to our well-being is a risk that is tempered by their benefits, as is the case with all technologies that embed the values of their designers, developers, and users. Digital twins are no different.\nHowever, that does not mean that I don’t think there is still a hugely rich and fascinating territory that demands exploration. While the topology and texture of this terrain is probably already fairly well understood, at least in the domain of digital well-being, due to the existence of many generalisable lessons and knowledge, there is always further value that can be created by more detailed examination and exploration.\nI hope I have encouraged some of you that such exploration is worthwhile. If not, then thank you at least for listening.\n\nFootnotes §\n\n\nBurr, C., Cristianini, N., &amp; Ladyman, J. (2018). An Analysis of the Interaction Between Intelligent Software Agents and Human Users. Minds and Machines, 28(4), 735–774. https://doi.org/10.1007/s11023-018-9479-0 ↩\n\n\nBurr, C., &amp; Cristianini, N. (2019). Can Machines Read our Minds? Minds and Machines, 29(3), 461–494. https://doi.org/10.1007/s11023-019-09497-4 ↩\n\n\nBurr, C., Taddeo, M., &amp; Floridi, L. (2020). The Ethics of Digital Well-Being: A Thematic Review. Science and Engineering Ethics. https://doi.org/10.1007/s11948-020-00175-8 ↩\n\n\nBurr, C., J. Morley, M. Taddeo, &amp; L. Floridi. (2020). Digital Psychiatry: Risks and Opportunities for Public Health and Wellbeing. IEEE Transactions on Technology and Society, 1(1), 21–33. https://doi.org/10.1109/TTS.2020.2977059 ↩\n\n\nBurr, C., &amp; Floridi, L. (Eds.). (2020). Ethics of Digital Well-Being: A Multidisciplinary Approach (Vol. 140). Springer International Publishing. https://doi.org/10.1007/978-3-030-50585-1 ↩\n\n\nZhuang, C., Liu, J., &amp; Xiong, H. (2018). Digital twin-based smart production management and control framework for the complex product assembly shop-floor. The International Journal of Advanced Manufacturing Technology, 96, 1149–1163. ↩\n\n\nThough see a recent proposal from our programme and collaborators for some initial thoughts on the development of a philosophical framework for digital twins. Wagg, D., Burr, C., Shepherd, J., Conti, Z. X., Enzer, M., &amp; Niederer, S. (2024). The philosophical foundations of digital twinning. Engineering Archive. https://doi.org/10.31224/3500 ↩\n\n\nSee the Microsoft Research project, Autogen, for further details on this idea. ↩\n\n\nLanier, J. (1995). Agents of alienation. Interactions, 2(3), 66–72. ↩\n\n\nKorenhof, P., Blok, V., &amp; Kloppenburg, S. (2021). Steering Representations—Towards a Critical Understanding of Digital Twins. Philosophy &amp; Technology, 34(4), 1751–1773. https://doi.org/10.1007/s13347-021-00484-1 ↩\n\n\n"},"teaching-materials":{"title":"Teaching Materials","links":[],"tags":["list"],"content":"\n\n                  \n                  About this Page \n                  \n                \nOn this page you can find information and links to some of my teaching materials.\n\nTuring Commons §\n\nThe Turing Commons is an online platform I created, in collaboration with others at the Alan Turing Institute, to support open dialogue and reflection about the responsible design, development, and deployment of data-driven technologies. On this platform you can find a range of freely available teaching materials, including the following courses:\n\nResponsible Research and Innovation\nPublic Engagement of Data Science and AI\nAI Ethics &amp; Governance\n\nA Citizen’s Guide to Data: Ethical, Social, and Legal Issues §\nThis guide was produced to support specific activities associated with a Resident’s Panel, organised and run by Camden City Council and Involve. The content was designed to support the participants with understanding some of the ethical, legal, and social issues surrounding data use by local government.\nBurr, C. (2021). A Citizen’s Guide to Data: Ethical, Social, and Legal Issues. Zenodo. https://doi.org/10.5281/ZENODO.5568861"}}